{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe58c9960d91433c93acca47eff2f565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import findspark                                              #Import library to Search for Spark Installation  \n",
    "\n",
    "findspark.init()                                              #Search Spark Installation\n",
    "\n",
    "import pyspark                                                #Only run after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession                          #Import of Spark Session\n",
    "from pyspark import SparkContext as spark                     #Import the Regular Spark Contex \n",
    "from pyspark.sql import SQLContext                            #Import the SQL Spark Contex \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import os\n",
    "\n",
    "sc = spark.sparkContext                                       #Initialize Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERDAU_BUCKET = os.environ['GERDAU_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPG_OUTPUT_BUCKET = os.environ['INTEGRATION_INPUT_BUCKET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Profitability COPA BW\n",
    "GERDAU_BUCKET_INPUT_BW_PARQUET = \"global/co/co_pa_dem/part-00000-1eb8fa1b-2207-438d-ba71-42b166869afb-c000.snappy.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Profitability COPA BW\n",
    "GERDAU_BUCKET_INPUT_BW = \"global/co/co_pa_dem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the Data Frame to the Smart Pricing bucket\n",
    "SPG_OUTPUT_BUCKET_BW = \"SPG_FACTS/SPG_BW/SPG_COPA.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating method to remove characters from string\n",
    "def remove_some_chars(col_name):\n",
    "    removed_chars = (\".\")\n",
    "    regexp = \"|\".join('\\{0}'.format(i) for i in removed_chars)\n",
    "    return regexp_replace(col_name, regexp, \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Profitability Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading parquet file from the Gerdau Analytics Data Lake\n",
    "rawReportSNAPPY = spark.read.parquet(\"s3://\"+GERDAU_BUCKET+\"/\"+GERDAU_BUCKET_INPUT_BW_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReportSNAPPY.write.partitionBy(144);\n",
    "rawReportSNAPPY = rawReportSNAPPY.repartition(144);\n",
    "rawReportSNAPPY.persist(pyspark.StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReport = spark.read.parquet(\"s3://\"+GERDAU_BUCKET+\"/\"+GERDAU_BUCKET_INPUT_BW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReport.write.partitionBy(144);\n",
    "rawReport = rawReport.repartition(144);\n",
    "rawReport.persist(pyspark.StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReport = rawReport.drop('year')\n",
    "rawReport = rawReport.drop('month')\n",
    "rawReport = rawReport.drop('day')\n",
    "rawReport = rawReport.unionByName(rawReportSNAPPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReport = rawReport.drop('year')\n",
    "rawReport = rawReport.drop('month')\n",
    "rawReport = rawReport.drop('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReport= rawReport.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReport.write.partitionBy(144);\n",
    "rawReport = rawReport.repartition(144);\n",
    "rawReport.persist(pyspark.StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in rawReport.columns:\n",
    "    rawReport = rawReport.withColumnRenamed(col, col.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawReport=rawReport.filter(rawReport['0CURRENCY'].like('BRL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the existing column with a corrected one\n",
    "rawReport = rawReport.withColumn('DI_EXTRACT_TIME', remove_some_chars('DI_EXTRACT_TIME'))\\\n",
    "                    .withColumn('GTC100025', remove_some_chars('GTC100025'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ordering by the most recent Processing (Extract) Timestamp\n",
    "rawReport=rawReport.withColumn('DI_EXTRACT_TIME', from_unixtime(unix_timestamp('DI_EXTRACT_TIME', 'yyyy/MM/dd')))\\\n",
    "                   .withColumn('GTC100025', from_unixtime(unix_timestamp('GTC100025', 'yyyy/MM/dd')))\n",
    "                    \n",
    "rawReport = rawReport.sort(\"DI_EXTRACT_TIME\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 24 months worth of data\n",
    "rawReport=rawReport.where(rawReport['GTC100025']>=add_months(current_date(),-24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Key using 8 fields\n",
    "# GTC100257 -> Tipo de Documento de Venda (Sales Document Type)\n",
    "# GTC101667 -> Número do Documento de Faturamento (Billing Document Number)\n",
    "# GTC101671 -> Item do Documento de Faturamento (Billing Document Item)\n",
    "# GTC100137 -> Tipo do Documento de Faturamento (Billing Document Type)\n",
    "# GTC100511 -> Organização de Vendas (Sales Organization)\n",
    "# GTC100255 -> Número do Documento de Venda (Sales Document Number)\n",
    "# GTC100504 -> Item de Documento de Venda (Sales Document Item)\n",
    "# 0FISCPER  -> Período (Period)\n",
    "\n",
    "rawReport = rawReport.withColumn('BW_KEY', \n",
    "                                 concat(col('GTC100257'),\n",
    "                                        col('GTC101667'), \n",
    "                                        col('GTC101671'), \n",
    "                                        col('GTC100137'),\n",
    "                                        col('GTC100511'),\n",
    "                                        col('GTC100255'),\n",
    "                                        col('GTC100504'),\n",
    "                                        col('0FISCPER')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Duplicates using the created Key as Parameter \n",
    "ProcessedReport = rawReport.dropDuplicates(['BW_KEY']).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedReport.write.partitionBy(\"GTC100362\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedReport = ProcessedReport.repartition(\"GTC100362\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedReport.persist(pyspark.StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the Data Frame to the Smart Pricing bucket\n",
    "ProcessedReport.write.parquet(\"s3a://\"+SPG_OUTPUT_BUCKET+\"/\"+SPG_OUTPUT_BUCKET_BW, mode = \"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
